# Speech-to-Text App Exploration Document

## 概述

本文档探索如何使用 ElevenLabs Scribe v2 Realtime API 和 Tauri 2 构建一个类似 [Wispr Flow](https://wisprflow.ai/) 的语音转文字工具。

## 1. ElevenLabs Scribe v2 Realtime API

### 1.1 API 概述

Scribe v2 Realtime 是 ElevenLabs 提供的实时语音转文字服务：
- **延迟**: 约 150ms（优化条件下可达 30-80ms）
- **支持语言**: 90+ 种语言
- **连接方式**: WebSocket

**官方文档**:
- [Scribe v2 Realtime API Reference](https://elevenlabs.io/docs/api-reference/speech-to-text/v-1-speech-to-text-realtime)
- [Server-side Streaming Guide](https://elevenlabs.io/docs/developers/guides/cookbooks/speech-to-text/realtime/server-side-streaming)

### 1.2 WebSocket 连接

```
wss://api.elevenlabs.io/v1/speech-to-text/realtime
```

#### 认证方式

1. **Header 认证** (服务端使用):
   ```
   xi-api-key: your-api-key
   ```

2. **Token 认证** (客户端使用):
   ```
   wss://api.elevenlabs.io/v1/speech-to-text/realtime?token=<single-use-token>
   ```
   Token 需通过 `/v1/speech-to-text/realtime/token` 端点获取。

### 1.3 消息格式

#### Session 配置响应

```json
{
  "message_type": "session_started",
  "session_id": "abc123",
  "config": {
    "sample_rate": 16000,
    "audio_format": "pcm_16000",
    "language_code": "en",
    "model_id": "scribe_v2_realtime",
    "vad_commit_strategy": false,
    "vad_silence_threshold_secs": 1.5,
    "include_timestamps": true
  }
}
```

#### 发送音频数据

```json
{
  "message_type": "input_audio_chunk",
  "audio_base_64": "<base64_encoded_pcm_audio>",
  "commit": false,
  "sample_rate": 16000
}
```

#### 接收转录结果

```json
{
  "message_type": "partial_transcript",
  "text": "Hello world"
}
```

### 1.4 TypeScript 实现示例

```typescript
// elevenlabs-stt-realtime.ts
import WebSocket from 'ws';

interface ScribeConfig {
  apiKey: string;
  sampleRate?: number;
  languageCode?: string;
  vadCommitStrategy?: boolean;
}

interface TranscriptMessage {
  message_type: 'partial_transcript' | 'committed_transcript';
  text: string;
  timestamp?: number;
}

class ElevenLabsRealtimeSTT {
  private ws: WebSocket | null = null;
  private config: ScribeConfig;

  constructor(config: ScribeConfig) {
    this.config = {
      sampleRate: 16000,
      languageCode: 'en',
      vadCommitStrategy: true,
      ...config
    };
  }

  async connect(): Promise<void> {
    return new Promise((resolve, reject) => {
      const url = 'wss://api.elevenlabs.io/v1/speech-to-text/realtime';

      this.ws = new WebSocket(url, {
        headers: {
          'xi-api-key': this.config.apiKey
        }
      });

      this.ws.on('open', () => {
        console.log('Connected to ElevenLabs Scribe v2 Realtime');
        // 发送配置
        this.sendConfig();
        resolve();
      });

      this.ws.on('error', (error) => {
        reject(error);
      });
    });
  }

  private sendConfig(): void {
    if (!this.ws) return;

    const config = {
      message_type: 'session_config',
      sample_rate: this.config.sampleRate,
      language_code: this.config.languageCode,
      vad_commit_strategy: this.config.vadCommitStrategy
    };

    this.ws.send(JSON.stringify(config));
  }

  sendAudioChunk(audioBuffer: Buffer, commit: boolean = false): void {
    if (!this.ws || this.ws.readyState !== WebSocket.OPEN) return;

    const message = {
      message_type: 'input_audio_chunk',
      audio_base_64: audioBuffer.toString('base64'),
      commit: commit,
      sample_rate: this.config.sampleRate
    };

    this.ws.send(JSON.stringify(message));
  }

  onTranscript(callback: (transcript: TranscriptMessage) => void): void {
    if (!this.ws) return;

    this.ws.on('message', (data: Buffer) => {
      try {
        const message = JSON.parse(data.toString());
        if (message.message_type === 'partial_transcript' ||
            message.message_type === 'committed_transcript') {
          callback(message);
        }
      } catch (e) {
        console.error('Failed to parse message:', e);
      }
    });
  }

  commit(): void {
    if (!this.ws) return;

    this.ws.send(JSON.stringify({
      message_type: 'commit'
    }));
  }

  disconnect(): void {
    if (this.ws) {
      this.ws.close();
      this.ws = null;
    }
  }
}

export { ElevenLabsRealtimeSTT, ScribeConfig, TranscriptMessage };
```

### 1.5 浏览器端音频捕获

使用 AudioWorklet 捕获 PCM 16kHz 音频：

```typescript
// audio-capture.ts
class AudioCapture {
  private audioContext: AudioContext | null = null;
  private mediaStream: MediaStream | null = null;
  private workletNode: AudioWorkletNode | null = null;
  private onAudioChunk: ((chunk: ArrayBuffer) => void) | null = null;

  async start(onChunk: (chunk: ArrayBuffer) => void): Promise<void> {
    this.onAudioChunk = onChunk;

    // 创建 16kHz 的 AudioContext
    this.audioContext = new AudioContext({ sampleRate: 16000 });

    // 获取麦克风权限
    this.mediaStream = await navigator.mediaDevices.getUserMedia({
      audio: {
        channelCount: 1,
        sampleRate: 16000,
        echoCancellation: true,
        noiseSuppression: true
      }
    });

    // 加载 AudioWorklet 处理器
    await this.audioContext.audioWorklet.addModule('/pcm-processor.js');

    // 创建处理节点
    const source = this.audioContext.createMediaStreamSource(this.mediaStream);
    this.workletNode = new AudioWorkletNode(this.audioContext, 'pcm-processor');

    // 接收 PCM 数据
    this.workletNode.port.onmessage = (event) => {
      if (this.onAudioChunk) {
        this.onAudioChunk(event.data.pcmData);
      }
    };

    source.connect(this.workletNode);
    // 不连接到 destination，避免回声
  }

  stop(): void {
    if (this.workletNode) {
      this.workletNode.disconnect();
      this.workletNode = null;
    }

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach(track => track.stop());
      this.mediaStream = null;
    }

    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }
  }
}
```

```javascript
// pcm-processor.js (AudioWorklet)
class PCMProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
    this.bufferSize = 4096; // ~256ms at 16kHz
    this.buffer = new Float32Array(this.bufferSize);
    this.bufferIndex = 0;
  }

  process(inputs, outputs, parameters) {
    const input = inputs[0];
    if (!input || !input[0]) return true;

    const samples = input[0];

    for (let i = 0; i < samples.length; i++) {
      this.buffer[this.bufferIndex++] = samples[i];

      if (this.bufferIndex >= this.bufferSize) {
        // 转换为 16-bit PCM
        const pcm16 = new Int16Array(this.bufferSize);
        for (let j = 0; j < this.bufferSize; j++) {
          const s = Math.max(-1, Math.min(1, this.buffer[j]));
          pcm16[j] = s < 0 ? s * 0x8000 : s * 0x7FFF;
        }

        this.port.postMessage({ pcmData: pcm16.buffer }, [pcm16.buffer]);
        this.buffer = new Float32Array(this.bufferSize);
        this.bufferIndex = 0;
      }
    }

    return true;
  }
}

registerProcessor('pcm-processor', PCMProcessor);
```

## 2. Tauri 2 应用架构设计

### 2.1 技术栈

- **前端**: TypeScript + Svelte/React
- **后端**: Rust (Tauri 2)
- **关键插件**:
  - `tauri-plugin-global-shortcut` - 全局快捷键
  - `tauri-plugin-mic-recorder` - 麦克风录音 (可选)
  - `enigo` / `rdev` - 键盘模拟输入

### 2.2 应用架构

```
┌─────────────────────────────────────────────────────────────┐
│                       Tauri 2 App                           │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────────────┐    ┌─────────────────────────────┐ │
│  │     System Tray     │    │      Global Hotkey          │ │
│  │   (状态指示/菜单)    │    │   (cmd+shift+\)            │ │
│  └─────────────────────┘    └─────────────────────────────┘ │
├─────────────────────────────────────────────────────────────┤
│                       Rust Backend                          │
│  ┌─────────────────┐  ┌─────────────┐  ┌─────────────────┐  │
│  │  Audio Capture  │  │  WebSocket  │  │  Text Insertion │  │
│  │  (cpal/rodio)   │  │   Client    │  │  (enigo/rdev)   │  │
│  └─────────────────┘  └─────────────┘  └─────────────────┘  │
├─────────────────────────────────────────────────────────────┤
│                     WebView Frontend                        │
│  ┌─────────────────────────────────────────────────────────┐│
│  │  - 状态显示 (Recording / Idle)                          ││
│  │  - 实时转录文本预览                                      ││
│  │  - 设置界面 (API Key, 语言选择等)                       ││
│  └─────────────────────────────────────────────────────────┘│
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
              ┌───────────────────────────────┐
              │  ElevenLabs Scribe v2 API     │
              │  (WebSocket)                  │
              └───────────────────────────────┘
```

### 2.3 项目结构

```
speech-app/
├── src-tauri/
│   ├── Cargo.toml
│   ├── src/
│   │   ├── main.rs
│   │   ├── lib.rs
│   │   ├── tray.rs           # System tray 管理
│   │   ├── hotkey.rs         # 全局快捷键处理
│   │   ├── audio.rs          # 音频捕获
│   │   ├── transcriber.rs    # ElevenLabs WebSocket 客户端
│   │   ├── text_input.rs     # 文本插入/剪贴板
│   │   └── state.rs          # 应用状态管理
│   └── tauri.conf.json
├── src/
│   ├── App.svelte
│   ├── lib/
│   │   ├── stores/
│   │   │   └── transcription.ts
│   │   └── components/
│   │       ├── StatusIndicator.svelte
│   │       └── TranscriptPreview.svelte
│   └── main.ts
├── package.json
└── vite.config.ts
```

### 2.4 核心 Rust 实现

#### 2.4.1 Cargo.toml 依赖

```toml
[dependencies]
tauri = { version = "2", features = ["tray-icon"] }
tauri-plugin-global-shortcut = "2"
tokio = { version = "1", features = ["full"] }
tokio-tungstenite = { version = "0.21", features = ["native-tls"] }
futures-util = "0.3"
cpal = "0.15"               # 音频捕获
enigo = "0.2"               # 键盘模拟
arboard = "3"               # 剪贴板
serde = { version = "1", features = ["derive"] }
serde_json = "1"
base64 = "0.21"
```

#### 2.4.2 System Tray 实现

```rust
// src-tauri/src/tray.rs
use tauri::{
    tray::{TrayIcon, TrayIconBuilder, MouseButton, MouseButtonState},
    menu::{Menu, MenuItem},
    Manager, Runtime,
};

pub fn create_tray<R: Runtime>(app: &tauri::AppHandle<R>) -> tauri::Result<TrayIcon<R>> {
    let quit = MenuItem::with_id(app, "quit", "Quit", true, None::<&str>)?;
    let toggle = MenuItem::with_id(app, "toggle", "Toggle Recording (⌘⇧\\)", true, None::<&str>)?;
    let settings = MenuItem::with_id(app, "settings", "Settings...", true, None::<&str>)?;

    let menu = Menu::with_items(app, &[&toggle, &settings, &quit])?;

    let tray = TrayIconBuilder::new()
        .icon(app.default_window_icon().unwrap().clone())
        .menu(&menu)
        .menu_on_left_click(false)
        .tooltip("Speech-to-Text")
        .on_menu_event(|app, event| {
            match event.id.as_ref() {
                "quit" => app.exit(0),
                "toggle" => {
                    // 触发录音切换
                    app.emit("toggle-recording", ()).ok();
                }
                "settings" => {
                    // 打开设置窗口
                    if let Some(window) = app.get_webview_window("main") {
                        window.show().ok();
                        window.set_focus().ok();
                    }
                }
                _ => {}
            }
        })
        .build(app)?;

    Ok(tray)
}

pub fn update_tray_icon(tray: &TrayIcon, recording: bool) {
    // 根据状态更新图标
    let icon_path = if recording { "icons/recording.png" } else { "icons/idle.png" };
    // tray.set_icon(Some(Icon::File(icon_path.into()))).ok();
}
```

#### 2.4.3 全局快捷键

```rust
// src-tauri/src/hotkey.rs
use tauri_plugin_global_shortcut::{Builder, Code, GlobalShortcutExt, Modifiers, Shortcut, ShortcutState};

pub fn setup_global_shortcut(app: &tauri::App) -> tauri::Result<()> {
    let shortcut = Shortcut::new(Some(Modifiers::SUPER | Modifiers::SHIFT), Code::Backslash);

    app.handle().plugin(
        Builder::new()
            .with_handler(move |app, shortcut, event| {
                if event.state == ShortcutState::Pressed {
                    // 触发录音切换
                    app.emit("toggle-recording", ()).ok();
                }
            })
            .build(),
    )?;

    app.global_shortcut().register(shortcut)?;

    Ok(())
}
```

#### 2.4.4 音频捕获 (使用 cpal)

```rust
// src-tauri/src/audio.rs
use cpal::traits::{DeviceTrait, HostTrait, StreamTrait};
use std::sync::mpsc;
use std::sync::Arc;
use std::sync::atomic::{AtomicBool, Ordering};

pub struct AudioCapture {
    stream: Option<cpal::Stream>,
    is_recording: Arc<AtomicBool>,
}

impl AudioCapture {
    pub fn new() -> Self {
        Self {
            stream: None,
            is_recording: Arc::new(AtomicBool::new(false)),
        }
    }

    pub fn start(&mut self, on_chunk: impl Fn(Vec<i16>) + Send + 'static) -> Result<(), String> {
        let host = cpal::default_host();
        let device = host.default_input_device()
            .ok_or("No input device available")?;

        let config = cpal::StreamConfig {
            channels: 1,
            sample_rate: cpal::SampleRate(16000),
            buffer_size: cpal::BufferSize::Fixed(4096),
        };

        let is_recording = self.is_recording.clone();
        is_recording.store(true, Ordering::SeqCst);

        let stream = device.build_input_stream(
            &config,
            move |data: &[f32], _| {
                if is_recording.load(Ordering::SeqCst) {
                    // 转换为 16-bit PCM
                    let pcm16: Vec<i16> = data.iter()
                        .map(|&s| {
                            let clamped = s.max(-1.0).min(1.0);
                            (clamped * 32767.0) as i16
                        })
                        .collect();
                    on_chunk(pcm16);
                }
            },
            |err| eprintln!("Audio stream error: {}", err),
            None,
        ).map_err(|e| e.to_string())?;

        stream.play().map_err(|e| e.to_string())?;
        self.stream = Some(stream);

        Ok(())
    }

    pub fn stop(&mut self) {
        self.is_recording.store(false, Ordering::SeqCst);
        self.stream = None;
    }

    pub fn is_recording(&self) -> bool {
        self.is_recording.load(Ordering::SeqCst)
    }
}
```

#### 2.4.5 WebSocket 转录客户端

```rust
// src-tauri/src/transcriber.rs
use futures_util::{SinkExt, StreamExt};
use tokio_tungstenite::{connect_async, tungstenite::Message};
use serde::{Deserialize, Serialize};
use base64::{Engine as _, engine::general_purpose::STANDARD as BASE64};

#[derive(Serialize)]
struct AudioChunkMessage {
    message_type: String,
    audio_base_64: String,
    commit: bool,
    sample_rate: u32,
}

#[derive(Deserialize)]
pub struct TranscriptMessage {
    pub message_type: String,
    pub text: Option<String>,
}

pub struct Transcriber {
    api_key: String,
}

impl Transcriber {
    pub fn new(api_key: String) -> Self {
        Self { api_key }
    }

    pub async fn connect(
        &self,
        mut audio_rx: tokio::sync::mpsc::Receiver<Vec<i16>>,
        transcript_tx: tokio::sync::mpsc::Sender<String>,
    ) -> Result<(), Box<dyn std::error::Error>> {
        let url = "wss://api.elevenlabs.io/v1/speech-to-text/realtime";

        let request = http::Request::builder()
            .uri(url)
            .header("xi-api-key", &self.api_key)
            .header("Host", "api.elevenlabs.io")
            .header("Connection", "Upgrade")
            .header("Upgrade", "websocket")
            .header("Sec-WebSocket-Version", "13")
            .header("Sec-WebSocket-Key", tungstenite::handshake::client::generate_key())
            .body(())?;

        let (ws_stream, _) = connect_async(request).await?;
        let (mut write, mut read) = ws_stream.split();

        // 发送配置
        let config = serde_json::json!({
            "message_type": "session_config",
            "sample_rate": 16000,
            "language_code": "en",
            "vad_commit_strategy": true
        });
        write.send(Message::Text(config.to_string())).await?;

        // 处理接收消息的任务
        let tx = transcript_tx.clone();
        let read_task = tokio::spawn(async move {
            while let Some(msg) = read.next().await {
                if let Ok(Message::Text(text)) = msg {
                    if let Ok(transcript) = serde_json::from_str::<TranscriptMessage>(&text) {
                        if let Some(text) = transcript.text {
                            tx.send(text).await.ok();
                        }
                    }
                }
            }
        });

        // 发送音频数据的任务
        let write_task = tokio::spawn(async move {
            while let Some(pcm_data) = audio_rx.recv().await {
                let bytes: Vec<u8> = pcm_data.iter()
                    .flat_map(|&s| s.to_le_bytes())
                    .collect();

                let msg = AudioChunkMessage {
                    message_type: "input_audio_chunk".to_string(),
                    audio_base_64: BASE64.encode(&bytes),
                    commit: false,
                    sample_rate: 16000,
                };

                if write.send(Message::Text(serde_json::to_string(&msg).unwrap())).await.is_err() {
                    break;
                }
            }
        });

        tokio::select! {
            _ = read_task => {},
            _ = write_task => {},
        }

        Ok(())
    }
}
```

#### 2.4.6 文本插入模块

```rust
// src-tauri/src/text_input.rs
use enigo::{Enigo, Key, Keyboard, Settings};
use arboard::Clipboard;
use std::thread;
use std::time::Duration;

pub struct TextInserter {
    enigo: Enigo,
    clipboard: Clipboard,
}

impl TextInserter {
    pub fn new() -> Result<Self, Box<dyn std::error::Error>> {
        Ok(Self {
            enigo: Enigo::new(&Settings::default())?,
            clipboard: Clipboard::new()?,
        })
    }

    /// 尝试在当前光标位置输入文本
    /// 如果失败，将文本复制到剪贴板
    pub fn insert_text(&mut self, text: &str) -> Result<InsertResult, Box<dyn std::error::Error>> {
        // 方法1: 直接键盘输入
        // 注意: enigo.text() 在某些应用中可能失败

        // 方法2: 通过剪贴板粘贴 (更可靠)
        let original_clipboard = self.clipboard.get_text().ok();

        self.clipboard.set_text(text)?;

        // 短暂延迟确保剪贴板已更新
        thread::sleep(Duration::from_millis(50));

        // 模拟 Cmd+V (macOS) 粘贴
        self.enigo.key(Key::Meta, enigo::Direction::Press)?;
        thread::sleep(Duration::from_millis(10));
        self.enigo.key(Key::Unicode('v'), enigo::Direction::Click)?;
        thread::sleep(Duration::from_millis(10));
        self.enigo.key(Key::Meta, enigo::Direction::Release)?;

        // 恢复原始剪贴板内容
        if let Some(original) = original_clipboard {
            thread::sleep(Duration::from_millis(100));
            self.clipboard.set_text(&original)?;
        }

        Ok(InsertResult::Inserted)
    }

    /// 仅复制到剪贴板
    pub fn copy_to_clipboard(&mut self, text: &str) -> Result<(), Box<dyn std::error::Error>> {
        self.clipboard.set_text(text)?;
        Ok(())
    }
}

pub enum InsertResult {
    Inserted,
    CopiedToClipboard,
}
```

#### 2.4.7 主程序整合

```rust
// src-tauri/src/main.rs
mod tray;
mod hotkey;
mod audio;
mod transcriber;
mod text_input;

use std::sync::{Arc, Mutex};
use tauri::Manager;
use tokio::sync::mpsc;

struct AppState {
    is_recording: bool,
    transcript_buffer: String,
    audio_tx: Option<mpsc::Sender<Vec<i16>>>,
}

#[tauri::command]
async fn toggle_recording(
    state: tauri::State<'_, Arc<Mutex<AppState>>>,
    app: tauri::AppHandle,
) -> Result<bool, String> {
    let mut state = state.lock().map_err(|e| e.to_string())?;

    state.is_recording = !state.is_recording;

    if state.is_recording {
        // 开始录音
        app.emit("recording-started", ()).ok();
    } else {
        // 停止录音并插入文本
        let text = state.transcript_buffer.clone();
        state.transcript_buffer.clear();

        if !text.is_empty() {
            // 尝试插入文本
            let mut inserter = text_input::TextInserter::new()
                .map_err(|e| e.to_string())?;

            match inserter.insert_text(&text) {
                Ok(_) => app.emit("text-inserted", &text).ok(),
                Err(_) => {
                    inserter.copy_to_clipboard(&text).ok();
                    app.emit("text-copied", &text).ok();
                }
            };
        }

        app.emit("recording-stopped", ()).ok();
    }

    Ok(state.is_recording)
}

fn main() {
    tauri::Builder::default()
        .plugin(tauri_plugin_global_shortcut::init())
        .setup(|app| {
            // 创建系统托盘
            tray::create_tray(app.handle())?;

            // 设置全局快捷键
            hotkey::setup_global_shortcut(app)?;

            // 初始化状态
            app.manage(Arc::new(Mutex::new(AppState {
                is_recording: false,
                transcript_buffer: String::new(),
                audio_tx: None,
            })));

            // 窗口启动时隐藏，仅显示托盘
            if let Some(window) = app.get_webview_window("main") {
                window.hide()?;
            }

            Ok(())
        })
        .invoke_handler(tauri::generate_handler![toggle_recording])
        .run(tauri::generate_context!())
        .expect("error while running tauri application");
}
```

### 2.5 前端实现 (Svelte)

```svelte
<!-- src/App.svelte -->
<script lang="ts">
  import { onMount } from 'svelte';
  import { listen } from '@tauri-apps/api/event';
  import { invoke } from '@tauri-apps/api/core';

  let isRecording = false;
  let transcript = '';
  let statusMessage = 'Press ⌘⇧\\ to start';

  onMount(async () => {
    // 监听录音状态变化
    await listen('recording-started', () => {
      isRecording = true;
      statusMessage = 'Recording...';
    });

    await listen('recording-stopped', () => {
      isRecording = false;
      statusMessage = 'Press ⌘⇧\\ to start';
    });

    // 监听实时转录
    await listen<string>('transcript-update', (event) => {
      transcript = event.payload;
    });

    // 监听快捷键事件
    await listen('toggle-recording', async () => {
      await invoke('toggle_recording');
    });
  });
</script>

<main class="container">
  <div class="status" class:recording={isRecording}>
    <div class="indicator"></div>
    <span>{statusMessage}</span>
  </div>

  {#if transcript}
    <div class="transcript">
      <p>{transcript}</p>
    </div>
  {/if}

  <div class="hotkey-hint">
    <kbd>⌘</kbd> + <kbd>⇧</kbd> + <kbd>\</kbd>
  </div>
</main>

<style>
  .container {
    padding: 20px;
    text-align: center;
  }

  .status {
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 10px;
    margin-bottom: 20px;
  }

  .indicator {
    width: 12px;
    height: 12px;
    border-radius: 50%;
    background: #888;
  }

  .status.recording .indicator {
    background: #ff4444;
    animation: pulse 1s infinite;
  }

  @keyframes pulse {
    0%, 100% { opacity: 1; }
    50% { opacity: 0.5; }
  }

  .transcript {
    background: #f5f5f5;
    padding: 15px;
    border-radius: 8px;
    margin: 20px 0;
    max-height: 200px;
    overflow-y: auto;
  }

  .hotkey-hint {
    color: #666;
    font-size: 14px;
  }

  kbd {
    background: #eee;
    border: 1px solid #ccc;
    border-radius: 4px;
    padding: 2px 6px;
    font-family: monospace;
  }
</style>
```

### 2.6 Tauri 配置

```json
// src-tauri/tauri.conf.json
{
  "$schema": "https://schema.tauri.app/config/2",
  "productName": "SpeechToText",
  "identifier": "com.example.speech-to-text",
  "version": "0.1.0",
  "build": {
    "frontendDist": "../dist"
  },
  "app": {
    "withGlobalTauri": true,
    "trayIcon": {
      "iconPath": "icons/tray.png",
      "iconAsTemplate": true
    },
    "windows": [
      {
        "title": "Speech to Text",
        "width": 400,
        "height": 300,
        "visible": false,
        "resizable": false,
        "decorations": true,
        "alwaysOnTop": false
      }
    ],
    "security": {
      "capabilities": ["main-capability"]
    }
  },
  "plugins": {
    "global-shortcut": {}
  }
}
```

## 3. 关键技术挑战与解决方案

### 3.1 macOS 权限要求

应用需要以下权限：
- **麦克风访问**: 通过 `Info.plist` 配置 `NSMicrophoneUsageDescription`
- **辅助功能权限**: 用于键盘模拟输入，需要用户在 System Preferences > Privacy & Security > Accessibility 中授权

```xml
<!-- Info.plist additions -->
<key>NSMicrophoneUsageDescription</key>
<string>This app needs microphone access for speech-to-text transcription.</string>
```

### 3.2 文本插入策略

1. **首选方案**: 通过剪贴板 + Cmd+V 粘贴
   - 最可靠的方式
   - 需要暂存并恢复原剪贴板内容

2. **备选方案**: 检测当前应用是否支持输入
   - 使用 macOS Accessibility API 检查焦点元素
   - 如果不可输入，仅复制到剪贴板并通知用户

3. **enigo 已知问题**:
   - 在某些 macOS 版本和 Tauri 组合下可能崩溃
   - 需要在主线程调用 CGEvent API

### 3.3 音频格式处理

ElevenLabs Scribe v2 要求：
- **采样率**: 16kHz (推荐)
- **格式**: 16-bit PCM, 单声道
- **编码**: Base64 (通过 WebSocket 传输)

### 3.4 实时性优化

- **音频缓冲**: 0.1-1 秒的 chunk 大小
- **WebSocket**: 保持长连接，避免重复握手
- **VAD**: 启用语音活动检测，自动分段

## 4. 参考资料

### ElevenLabs
- [Scribe v2 Realtime API](https://elevenlabs.io/docs/api-reference/speech-to-text/v-1-speech-to-text-realtime)
- [Realtime Speech-to-Text Product Page](https://elevenlabs.io/realtime-speech-to-text)
- [ElevenLabs JS SDK](https://github.com/elevenlabs/elevenlabs-js)
- [ElevenLabs Examples](https://github.com/elevenlabs/elevenlabs-examples)

### Tauri 2
- [Tauri 2 System Tray](https://v2.tauri.app/learn/system-tray/)
- [Tauri Global Shortcut Plugin](https://v2.tauri.app/plugin/global-shortcut/)
- [tauri-plugin-mic-recorder](https://github.com/ayangweb/tauri-plugin-mic-recorder)

### 音频处理
- [MDN AudioWorklet](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_AudioWorklet)
- [cpal - Rust Audio Library](https://crates.io/crates/cpal)

### 键盘模拟
- [enigo - Keyboard/Mouse Simulation](https://docs.rs/enigo/latest/enigo/)
- [rdev - Input Event Library](https://github.com/Narsil/rdev)

### 竞品参考
- [Wispr Flow](https://wisprflow.ai/) - AI Voice Dictation
- [Wispr Flow Features](https://wisprflow.ai/features)

## 5. 下一步行动

1. **创建 Tauri 2 项目骨架**
2. **实现 System Tray 和全局快捷键**
3. **集成 ElevenLabs WebSocket 客户端**
4. **实现音频捕获和流式传输**
5. **实现文本插入功能**
6. **添加设置界面 (API Key 管理等)**
7. **测试和优化延迟**
